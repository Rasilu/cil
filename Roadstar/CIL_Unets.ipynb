{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XxbIj24th4T"
      },
      "source": [
        "# Combined Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQb7EB49hC3n"
      },
      "source": [
        "# Imports and Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9eOB60Fz1ah"
      },
      "outputs": [],
      "source": [
        "# initial imports\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "import gc\n",
        "import json\n",
        "import re\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtyiDCQ7_9FN"
      },
      "source": [
        "The below configuration cell can be set accordingly to reproduce any of our experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-04JGafgckn"
      },
      "outputs": [],
      "source": [
        "# some constants\n",
        "CONFIGURATION_NAME = 'unet3plusRS_bcedicemixcustom_nopre_allval'\n",
        "SEED = 123\n",
        "PATCH_SIZE = 16  # pixels per side of square patches\n",
        "CUTOFF = 0.25\n",
        "DATASETS = ['kaggle_data', 'new_data']\n",
        "BALANCE_DATASETS = False # if true each dataset will contribute a sample of equal size to the training set\n",
        "MAX_SAMPLE_SIZE = 0 # refers number of files to sample for each training iteration (0 => all files)\n",
        "VAL_SIZE = 200  # size of the validation set (number of images)\n",
        "VALIDATE_ONLY_KAGGLE = False # choose wether to only validate on kaggle data (mainly for early stopping)\n",
        "BATCH_SIZE = 8\n",
        "N_EPOCHS = 60\n",
        "USE_EARLY_STOPPING = True\n",
        "\n",
        "# filtering of the data, using min / max thresholds for percent of 1-labeled groundtruth pixels,\n",
        "# as well as a similarity threshold and which similarity network (alex or VGG) to compare against\n",
        "FILTER_DATA = True\n",
        "MIN_ROADS_PCT = 0.07\n",
        "MAX_ROADS_PCT = 0.7\n",
        "PREPROCESS_SIMILARITY_SCORE = 0.85\n",
        "SIMILARITIY_NETWORK = \"alex\"\n",
        "SIMILARITIY_IMAGE_TYPE = \"groundtruth\"\n",
        "\n",
        "# should never set both augmentation options to true!\n",
        "AUGMENT_DATA = False # creates a file for every 90 degree rotation for all images\n",
        "AUGMENT_ONLY_KAGGLE = True # does the augmentation, but only for the included kaggle data\n",
        "PREPROCESS_CONTRAST = False # normalize contrast\n",
        "PREPROCESS_RANDOMIZE = False # randomize contrast and brightness\n",
        "PREPROCESS_GAUSSIAN_BLUR_KERNEL_SIZE = 0 # Blurs images to increase edge detection\n",
        "POSTPROCESSING_MAJORITY = 4 # if over 0, rotates and predicts multiple times for each test image, taking the majority consensus\n",
        "POSTPROCESSING_MORPHOLOGICAL_ITERATIONS = 0 # if over 0, morph. postprocessing is used\n",
        "\n",
        "# a pre-processing feature extraction filter used for detecting tubular structures\n",
        "PREPROCESS_FRANGI = False\n",
        "\n",
        "# global constants used to implement full-scale skip connections in the U-Net3+\n",
        "GLOBAL_RES = 384\n",
        "GLOBAL_FIRST_CHANNELS = 64\n",
        "\n",
        "# our models and loss functions are defined throughout the course of this notebook,\n",
        "# so this dictionary is filled in as the notebook executes.\n",
        "MODEL = 'unet'  # model to be used\n",
        "LOSS_FN = 'bce' # loss function to be used \n",
        "# Dictionaries to store models and loss functions to make them accessible\n",
        "MODELS = {\n",
        "    'unet': None,             # basic UNet()\n",
        "    'sdunet': None,           # SDUNet\n",
        "    'attention_sdunet': None, # Attention UNet using SDC blocks\n",
        "    'unet3plusRS': None       # Our custom U-Net3+RS\n",
        "    }\n",
        "LOSS_FNS = {\n",
        "    'bce': None,                  # vanilla nn.BCELoss()\n",
        "    'soft_dice': None,            # soft dice loss\n",
        "    'custom_bce': None,           # a custom softening idea for the focal BCE, not used in final experiments due to poor performance\n",
        "    'focal_bce': None,            # vanilla focal BCE\n",
        "    'bce_dice_mix': None,         # mix of dice and focal BCE\n",
        "    'custom_bce_dice_mix': None,  # a custom softening idea for the focal BCE, not used in final experiments\n",
        "    'focal_dice_chi': None        # our final custom loss function, detailed in the report\n",
        "    }\n",
        "\n",
        "# training can be resumed from a loaded pytorch model checkpoint by setting this parameter\n",
        "LOAD_CHECKPOINT = None\n",
        "# this dictionary is used to create logging output for reproducability of any experiment\n",
        "HYPERPARAMETERS = {\n",
        "    'datasets': DATASETS,\n",
        "    'balance_datasets': BALANCE_DATASETS, \n",
        "    'train_set_size': 0, \n",
        "    'val_set_size': VAL_SIZE, \n",
        "    'n_epochs': N_EPOCHS,\n",
        "    'seed': SEED,\n",
        "    'model': MODEL,\n",
        "    'loss_fn': LOSS_FN,\n",
        "    'frangi': PREPROCESS_FRANGI\n",
        "    }\n",
        "STATE = {\n",
        "    'total_iterations': 0,\n",
        "    'current_iteration': 0,\n",
        "    'epoch': 0,\n",
        "    'time_trained': 0\n",
        "    }\n",
        "\n",
        "# enable some debugging prints\n",
        "DEBUG = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAY5CFxk10q1"
      },
      "outputs": [],
      "source": [
        "# globally set seeds for reproducibility\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT81ZUCXg-se"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJywZcKls-J_"
      },
      "source": [
        "## Functions\n",
        "Here we define various util functions to enable the loading, filtering and augmentation of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1MTqXvZO24m"
      },
      "outputs": [],
      "source": [
        "def load_from_path(path, isGroundtruth, sample = []):\n",
        "    if sample == []:\n",
        "        files = sorted(glob(path + '/*.png'))\n",
        "    else:\n",
        "        files = list(map(lambda basename: os.path.join(path, basename), sample))\n",
        "\n",
        "    if isGroundtruth:\n",
        "        img_stack = np.stack([np.array(Image.open(f).convert('1')) for f in files]).astype(np.float32)\n",
        "    else:\n",
        "        img_stack = np.stack([np.array(Image.open(f).convert('RGB')) for f in files]).astype(np.float32) / 255.\n",
        "        \n",
        "    print(f\"{len(files)} files loaded. Shape = {img_stack.shape}. Max Value = {img_stack.max()}\")\n",
        "    return img_stack\n",
        "\n",
        "def generate_sample(path, sample_size, already_sampled = []):\n",
        "    all_files = list(map(os.path.basename, sorted(glob(path + '/*.png'))))\n",
        "    sample = [f for f in all_files if f not in already_sampled]\n",
        "    if sample_size > 0 and len(sample) > sample_size:\n",
        "        sample = sorted(random.sample(sample, sample_size))\n",
        "    print(f\"Generated sample consisting of {len(sample)} files.\")\n",
        "    return sample\n",
        "\n",
        "def show_first_n(imgs, masks, n=5):\n",
        "    # visualizes the first n elements of a series of images and segmentation masks\n",
        "    imgs_to_draw = min(5, len(imgs))\n",
        "    fig, axs = plt.subplots(2, imgs_to_draw, figsize=(18.5, 6))\n",
        "    for i in range(imgs_to_draw):\n",
        "        axs[0, i].imshow(imgs[i])\n",
        "        axs[1, i].imshow(masks[i])\n",
        "        axs[0, i].set_title(f'Image {i}')\n",
        "        axs[1, i].set_title(f'Mask {i}')\n",
        "        axs[0, i].set_axis_off()\n",
        "        axs[1, i].set_axis_off()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70X-v7SGoCI9"
      },
      "outputs": [],
      "source": [
        "# returns filter which includes indices of images included in the dataset\n",
        "def filter_roads(masks):\n",
        "    # Get labels not reshaped (else everything same as image_to_patches)\n",
        "    n_images = masks.shape[0]  # number of images\n",
        "    h, w = masks.shape[1:3]  # shape of images\n",
        "    assert (h % PATCH_SIZE) + (w % PATCH_SIZE) == 0  # make sure images can be patched exactly\n",
        "\n",
        "    h_patches = h // PATCH_SIZE\n",
        "    w_patches = w // PATCH_SIZE\n",
        "\n",
        "    masks = masks.reshape((n_images, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE, -1))\n",
        "    masks = np.moveaxis(masks, 2, 3)\n",
        "    labels = np.mean(masks, (-1, -2, -3)) > CUTOFF  # compute labels\n",
        "    labels = labels.astype(np.float32)\n",
        "\n",
        "    filter = []\n",
        "\n",
        "    for i, d in enumerate(labels):\n",
        "        dt = d.reshape(-1)\n",
        "        if (sum(dt) / len(dt) <= MIN_ROADS_PCT or sum(dt) / len(dt) >= MAX_ROADS_PCT):\n",
        "            filter.append(i)\n",
        "\n",
        "    return filter\n",
        "\n",
        "# Load data with this: e.g. tTI = genfromtxt('cil/results/similarities/similaritiesAlexTrainTrainImages.csv', delimiter=',')\n",
        "def filter_similarities(data):\n",
        "    filter = []\n",
        "    for i, d in enumerate(data):\n",
        "        if (d.mean() >= PREPROCESS_SIMILARITY_SCORE):\n",
        "            filter.append(i)\n",
        "    return filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RuzLhnZkJVP"
      },
      "outputs": [],
      "source": [
        "# go through all the images and rotate them 3 times, producing an augmented dataset\n",
        "def produce_rotations(sourcePath, onlykaggle=False):\n",
        "    lst = os.listdir(sourcePath)\n",
        "    # can also choose to only augment kaggle data, giving this more weight\n",
        "    if onlykaggle:\n",
        "        lst = list(filter(lambda x: 'satimage' in x, lst))\n",
        "    for img_file in lst:\n",
        "        # initial unrotated\n",
        "        img_read = Image.open(sourcePath + '/' + img_file)\n",
        "        # filename ending indicates rotation amount\n",
        "        for i in range(1, 4):\n",
        "            img_read_rot = img_read.rotate(i * 90)\n",
        "            img_read_rot.save(sourcePath + '/' + img_file.replace('.png', '') + '_' + str(i * 90) + '.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxO6GG-dkUuY"
      },
      "source": [
        "## Import data from github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIpl4x6LsrS2"
      },
      "outputs": [],
      "source": [
        "start_download_data = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nkdG9SgAWSl"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Rasilu/cil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_znkNZSwa_c"
      },
      "outputs": [],
      "source": [
        "stop_download_data = time.time()\n",
        "print(f\"Time to load data: {round(stop_download_data - start_download_data, 2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H6VZoIQkdRc"
      },
      "source": [
        "## Create training and validation sets\n",
        "These cells implement our data filtering as described in our report. Importantly, we never filter the included kaggle data. At the start of every run, we also delete any images in the training folder that are also in the validation folder. We do this in order to enable faster testing, as this way the data does not have to be re-downloaded when restarting and re-running the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekmdsqyZo4GM"
      },
      "outputs": [],
      "source": [
        "# prepare datasets, by loading and filtering them\n",
        "print(\"Prepare datasets ...\")\n",
        "!mkdir -p datasets\n",
        "!rm -r datasets/*\n",
        "!cp -r cil/datasets/kaggle_data .\n",
        "for d in DATASETS:\n",
        "    !cp -r cil/datasets/{d} datasets\n",
        "    # never filter kaggle data!\n",
        "    if (not FILTER_DATA) or (d == 'kaggle_data'):\n",
        "        continue;\n",
        "    \n",
        "    # load the filenames of the given dataset\n",
        "    data_path = 'datasets/' + d\n",
        "    data_filtered_path = 'datasets/filtered/' + d\n",
        "    data_train_img_path = data_path + '/training/images'\n",
        "    data_train_msk_path = data_path + '/training/groundtruth'\n",
        "    dataset_img_files = sorted(glob(data_train_img_path + '/*.png'))\n",
        "    dataset_msk_files = sorted(glob(data_train_msk_path + '/*.png'))\n",
        "    data_basenames = list(map(os.path.basename, dataset_img_files))\n",
        "\n",
        "    # load the masks, check them against the similarity threshold and filter them\n",
        "    dataset_masks = load_from_path(data_train_msk_path, isGroundtruth=True)\n",
        "    filter_roads_exclude = filter_roads(dataset_masks)\n",
        "\n",
        "    file_name = \"similarities_\" + d + \"_kaggle_data_train_\" + SIMILARITIY_IMAGE_TYPE + \"_\" + SIMILARITIY_NETWORK + \".csv\"\n",
        "    csv_np = np.genfromtxt(\"cil/results/similaritiesForPipeline/\" + file_name, delimiter=',')\n",
        "    filter_similar_exclude = filter_similarities(csv_np)\n",
        "\n",
        "    filter_combined_exclude = [idx for idx in range(len(dataset_img_files)) if idx in filter_similar_exclude or idx in filter_roads_exclude]\n",
        "    print(f\"{len(filter_combined_exclude)} files deleted from {d}\")\n",
        "    filter_dataset_exclude = np.array(data_basenames)[filter_combined_exclude]\n",
        "    for basename in filter_dataset_exclude:\n",
        "        os.remove(f\"{data_train_img_path}/{basename}\")\n",
        "        os.remove(f\"{data_train_msk_path}/{basename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxgPRyCEiM3H"
      },
      "outputs": [],
      "source": [
        "def replace_name(s):\n",
        "    img_named = s.replace('.png', '')\n",
        "    return img_named\n",
        "\n",
        "# create training set\n",
        "print(\"Creating training set ...\")\n",
        "train_path = 'training'\n",
        "train_images_path = 'training/images'\n",
        "train_masks_path = 'training/groundtruth'\n",
        "!mkdir -p {train_path}\n",
        "!rm -r {train_path}/*\n",
        "!mkdir {train_path}/images\n",
        "!mkdir {train_path}/groundtruth\n",
        "# provides the option of cutting each dataset down to the same amount of samples as the smallest dataset\n",
        "if not BALANCE_DATASETS:\n",
        "    for d in DATASETS:\n",
        "        !cp -r datasets/{d}/training .\n",
        "else:\n",
        "    smallest_dataset = 1000000\n",
        "    for d in DATASETS:\n",
        "        dataset_size = len(glob(f\"datasets/{d}/training/images/*.png\"))\n",
        "        print(f\"{dataset_size} files in {d}\")\n",
        "        if dataset_size < smallest_dataset:\n",
        "            smallest_dataset = dataset_size\n",
        "    print(f\"Size of smallest dataset = {smallest_dataset} -> sample {smallest_dataset} files from each dataset\")\n",
        "    for d in DATASETS:\n",
        "        dataset_files = sorted(glob(f\"datasets/{d}/training/images/*.png\"))\n",
        "        dataset_sample = sorted(random.sample(dataset_files, smallest_dataset))\n",
        "        for img in dataset_sample:\n",
        "            mask = img.replace('images', 'groundtruth')\n",
        "            os.rename(img, img.replace(f\"datasets/{d}/training\", 'training'))\n",
        "            os.rename(mask, mask.replace(f\"datasets/{d}/training\", 'training'))\n",
        "\n",
        "# create validation set\n",
        "print(\"\\nCreating validation set ...\")\n",
        "!cp -r cil/datasets/kaggle_data .\n",
        "val_path = 'validation'\n",
        "!mkdir -p {val_path}\n",
        "!rm -r {val_path}/*\n",
        "!mkdir {val_path}/images\n",
        "!mkdir {val_path}/groundtruth\n",
        "kdata = \"\"\n",
        "# provides the option to only validate on kaggle data\n",
        "if VALIDATE_ONLY_KAGGLE:\n",
        "    kdata = \"kaggle_data/\"\n",
        "val_files = glob(kdata + \"training/images/*.png\")\n",
        "if VAL_SIZE >= 0 and VAL_SIZE <= len(val_files):\n",
        "    val_files = sorted(random.sample(val_files, VAL_SIZE))\n",
        "for img in val_files:\n",
        "    mask = img.replace('images', 'groundtruth')\n",
        "    os.rename(img, img.replace(kdata + 'training', 'validation'))\n",
        "    os.rename(mask, mask.replace(kdata + 'training', 'validation'))\n",
        "\n",
        "# remove training images (and variations) that are already contained in validation set\n",
        "# useful in the case that the notebook runtime is restarted without files being reset\n",
        "print(\"\\nRemoving files from validation set...\")\n",
        "train_files = sorted(glob(train_images_path +  '/*.png'))   # ex. ['validation/images/satimage_0.png', ...]\n",
        "train_basenames = list(map(os.path.basename, train_files))  # ex. ['satimage_0.png', 'satimage_1.png', ...]\n",
        "val_basenames = list(map(os.path.basename, val_files))      # ex. ['validation/images/satimage_2.png', ...]\n",
        "val_base = [os.path.splitext(split)[0] for split in val_basenames]  # ex. ['satimage_2.png', ...]\n",
        "val_base_re = '|'.join([vb for vb in val_base])             # ex. 'satimage_2|satimage_15|...|satimage_140'\n",
        "pat = re.compile(f'({val_base_re})\\\\D') # regex pattern: (val_base_re)[^0-9]\n",
        "count = 0\n",
        "for i in range(len(train_files)):\n",
        "    for j in range(len(val_files)):\n",
        "        if pat.match(train_basenames[i]): # matching regex to start of file basenames in training set\n",
        "            os.remove(train_files[i])\n",
        "            os.remove(train_files[i].replace('/images', '/groundtruth'))\n",
        "            count += 1\n",
        "            break\n",
        "print(str(count) + \" images removed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNeoWSHwgALW"
      },
      "outputs": [],
      "source": [
        "# exclusive augmentation of the kaggle data\n",
        "if AUGMENT_ONLY_KAGGLE:\n",
        "    produce_rotations(train_images_path, onlykaggle=True)\n",
        "    produce_rotations(train_masks_path, onlykaggle=True)\n",
        "# create augmented data if necessery\n",
        "if AUGMENT_DATA:\n",
        "    produce_rotations(train_images_path)\n",
        "    produce_rotations(train_masks_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXOMY_KZUoU6"
      },
      "outputs": [],
      "source": [
        "# sanity check: check size of training set and make sure the mask set is the same size\n",
        "train_img_set_size = len(glob(train_images_path + '/*.png'))\n",
        "train_msk_set_size = len(glob(train_masks_path + '/*.png'))\n",
        "assert(train_img_set_size == train_msk_set_size)\n",
        "HYPERPARAMETERS['train_set_size'] = train_img_set_size\n",
        "print(\"Training set size: \" + str(train_img_set_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVgQZELCO24r",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def image_to_patches(images, masks=None):\n",
        "    # takes in a 4D np.array containing images and (optionally) a 4D np.array containing the segmentation masks\n",
        "    # returns a 4D np.array with an ordered sequence of patches extracted from the image and (optionally) a np.array containing labels\n",
        "    n_images = images.shape[0]  # number of images\n",
        "    h, w = images.shape[1:3]  # shape of images\n",
        "    assert (h % PATCH_SIZE) + (w % PATCH_SIZE) == 0  # make sure images can be patched exactly\n",
        "\n",
        "    images = images[:,:,:,:3]\n",
        "    \n",
        "    h_patches = h // PATCH_SIZE\n",
        "    w_patches = w // PATCH_SIZE\n",
        "    \n",
        "    patches = images.reshape((n_images, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE, -1))\n",
        "    patches = np.moveaxis(patches, 2, 3)\n",
        "    patches = patches.reshape(-1, PATCH_SIZE, PATCH_SIZE, 3)\n",
        "    if masks is None:\n",
        "        return patches\n",
        "\n",
        "    masks = masks.reshape((n_images, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE, -1))\n",
        "    masks = np.moveaxis(masks, 2, 3)\n",
        "    labels = np.mean(masks, (-1, -2, -3)) > CUTOFF  # compute labels\n",
        "    labels = labels.reshape(-1).astype(np.float32)\n",
        "    return patches, labels\n",
        "\n",
        "\n",
        "def show_patched_image(patches, labels, h_patches=25, w_patches=25):\n",
        "    # reorders a set of patches in their original 2D shape and visualizes them\n",
        "    fig, axs = plt.subplots(h_patches, w_patches, figsize=(18.5, 18.5))\n",
        "    for i, (p, l) in enumerate(zip(patches, labels)):\n",
        "        # the np.maximum operation paints patches labeled as road red\n",
        "        axs[i // w_patches, i % w_patches].imshow(np.maximum(p, np.array([l.item(), 0., 0.])))\n",
        "        axs[i // w_patches, i % w_patches].set_axis_off()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOkXkVX7v6rp"
      },
      "outputs": [],
      "source": [
        "def create_submission(labels, test_filenames, submission_filename, test_path='test/images'):\n",
        "    \n",
        "    with open(submission_filename, 'w') as f:\n",
        "        f.write('id,prediction\\n')\n",
        "        for fn, patch_array in zip(sorted(test_filenames), labels):\n",
        "            img_number = int(re.search(r\"\\d+\", fn).group(0))\n",
        "            for i in range(patch_array.shape[0]):\n",
        "                for j in range(patch_array.shape[1]):\n",
        "                    f.write(\"{:03d}_{}_{},{}\\n\".format(img_number, j*PATCH_SIZE, i*PATCH_SIZE, int(patch_array[i, j])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQSklfhw1oHw"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4U-2F0XAkwx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "from skimage.filters import frangi\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \" + device)\n",
        "!nvidia-smi\n",
        "\n",
        "def np_to_tensor(x, device):\n",
        "    # allocates tensors from np.arrays\n",
        "    if device == 'cpu':\n",
        "        return torch.from_numpy(x).cpu()\n",
        "    else:\n",
        "        return torch.from_numpy(x).contiguous().pin_memory().to(device=device, non_blocking=True)\n",
        "\n",
        "\n",
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "    # dataset class that deals with loading the data and making it available by index.\n",
        "\n",
        "    def __init__(self, path, device, use_patches=True, resize_to=(400, 400), sample=[]):\n",
        "        self.path = path\n",
        "        self.device = device\n",
        "        self.use_patches = use_patches\n",
        "        self.resize_to=resize_to\n",
        "        self.x, self.y, self.n_samples = None, None, None\n",
        "        self._load_data(sample)\n",
        "        if PREPROCESS_CONTRAST:\n",
        "          self.deviation = np.std(self.x)\n",
        "\n",
        "    def _load_data(self, sample=[]):\n",
        "        self.x = load_from_path(os.path.join(self.path, 'images'), isGroundtruth=False, sample=sample)[:,:,:,:3]\n",
        "        self.y = np.ceil(load_from_path(os.path.join(self.path, 'groundtruth'), isGroundtruth=True, sample=sample))\n",
        "        if self.use_patches:  # split each image into patches\n",
        "            self.x, self.y = image_to_patches(self.x, self.y)\n",
        "        elif self.resize_to != (self.x.shape[1], self.x.shape[2]):  # resize images\n",
        "            self.x = np.stack([cv2.resize(img, dsize=self.resize_to) for img in self.x], 0)\n",
        "            self.y = np.stack([cv2.resize(mask, dsize=self.resize_to) for mask in self.y], 0)\n",
        "        self.x = np.moveaxis(self.x, -1, 1)  # pytorch works with CHW format instead of HWC\n",
        "        self.n_samples = len(self.x)\n",
        "\n",
        "    def _preprocess(self, x, y):\n",
        "        # various options for preprocessing can be configured at the start of the notebook\n",
        "        if PREPROCESS_CONTRAST:\n",
        "            x = (x - torch.mean(x)) / self.deviation\n",
        "        if PREPROCESS_RANDOMIZE:\n",
        "            jitter = torchvision.transforms.ColorJitter(brightness=0.1, contrast=0.3)\n",
        "            x = jitter.forward(x)\n",
        "        if PREPROCESS_GAUSSIAN_BLUR_KERNEL_SIZE > 0:\n",
        "            transform = transforms.Compose([transforms.GaussianBlur(kernel_size=PREPROCESS_GAUSSIAN_BLUR_KERNEL_SIZE)])\n",
        "            x = transform(x)\n",
        "        if PREPROCESS_FRANGI:\n",
        "            # filters only work on grayscale image, so it must first be converted\n",
        "            with torch.no_grad():\n",
        "                grayx = np.squeeze(transforms.Grayscale().forward(x).cpu().numpy(), axis=0)\n",
        "                stackx = frangi(grayx)\n",
        "                # now stack this information (feature extraction) onto the channel dimension of x\n",
        "                x = torch.cat([x, torch.unsqueeze(np_to_tensor(stackx.astype(np.float32), self.device), 0)], dim=0)\n",
        "        return x, y\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self._preprocess(np_to_tensor(self.x[item], self.device), np_to_tensor(self.y[[item]], self.device))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "\n",
        "def show_val_samples(x, y, y_hat, segmentation=False):\n",
        "    # training callback to show predictions on validation set\n",
        "    imgs_to_draw = min(5, len(x))\n",
        "    if x.shape[-2:] == y.shape[-2:]:  # segmentation\n",
        "        fig, axs = plt.subplots(3, imgs_to_draw, figsize=(18.5, 12))\n",
        "        for i in range(imgs_to_draw):\n",
        "            axs[0, i].imshow(np.moveaxis(x[i], 0, -1))\n",
        "            axs[1, i].imshow(np.concatenate([np.moveaxis(y_hat[i], 0, -1)] * 3, -1))\n",
        "            axs[2, i].imshow(np.concatenate([np.moveaxis(y[i], 0, -1)]*3, -1))\n",
        "            axs[0, i].set_title(f'Sample {i}')\n",
        "            axs[1, i].set_title(f'Predicted {i}')\n",
        "            axs[2, i].set_title(f'True {i}')\n",
        "            axs[0, i].set_axis_off()\n",
        "            axs[1, i].set_axis_off()\n",
        "            axs[2, i].set_axis_off()\n",
        "    else:  # classification\n",
        "        fig, axs = plt.subplots(1, imgs_to_draw, figsize=(18.5, 6))\n",
        "        for i in range(imgs_to_draw):\n",
        "            axs[i].imshow(np.moveaxis(x[i], 0, -1))\n",
        "            axs[i].set_title(f'True: {np.round(y[i]).item()}; Predicted: {np.round(y_hat[i]).item()}')\n",
        "            axs[i].set_axis_off()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEx4p0hwJyF-"
      },
      "source": [
        "# Models\n",
        "All Models outlined in our report are implemented here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFpPqQccO241"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    # a repeating structure composed of two convolutional layers with batch normalization and ReLU activations\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=3, padding=1),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.BatchNorm2d(out_ch),\n",
        "                                   nn.Conv2d(in_channels=out_ch, out_channels=out_ch, kernel_size=3, padding=1),\n",
        "                                   nn.ReLU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "        \n",
        "class UNet(nn.Module):\n",
        "    # UNet-like architecture for single class semantic segmentation.\n",
        "    def __init__(self, chs=(3,64,128,256,512,1024)):\n",
        "        super().__init__()\n",
        "        enc_chs = chs  # number of channels in the encoder\n",
        "        dec_chs = chs[::-1][:-1]  # number of channels in the decoder\n",
        "        self.enc_blocks = nn.ModuleList([Block(in_ch, out_ch) for in_ch, out_ch in zip(enc_chs[:-1], enc_chs[1:])])  # encoder blocks\n",
        "        self.pool = nn.MaxPool2d(2)  # pooling layer (can be reused as it will not be trained)\n",
        "        self.upconvs = nn.ModuleList([nn.ConvTranspose2d(in_ch, out_ch, 2, 2) for in_ch, out_ch in zip(dec_chs[:-1], dec_chs[1:])])  # deconvolution\n",
        "        self.dec_blocks = nn.ModuleList([Block(in_ch, out_ch) for in_ch, out_ch in zip(dec_chs[:-1], dec_chs[1:])])  # decoder blocks\n",
        "        self.head = nn.Sequential(nn.Conv2d(dec_chs[-1], 1, 1), nn.Sigmoid()) # 1x1 convolution for producing the output\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encode\n",
        "        enc_features = []\n",
        "        for block in self.enc_blocks[:-1]:\n",
        "            x = block(x)  # pass through the block\n",
        "            enc_features.append(x)  # save features for skip connections\n",
        "            x = self.pool(x)  # decrease resolution\n",
        "        x = self.enc_blocks[-1](x)\n",
        "        # decode\n",
        "        for block, upconv, feature in zip(self.dec_blocks, self.upconvs, enc_features[::-1]):\n",
        "            x = upconv(x)  # increase resolution\n",
        "            x = torch.cat([x, feature], dim=1)  # concatenate skip features\n",
        "            x = block(x)  # pass through the block\n",
        "        return self.head(x)  # reduce to 1 channel\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtC70A78oo-3"
      },
      "outputs": [],
      "source": [
        "class StackedDilConv(nn.Module):\n",
        "    # an implementation of the SDC block, providing improved spatial resolution at different \n",
        "    # granularities by using dilations of various sizes accross its filter channels.\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.hlf = nn.Sequential(nn.Conv2d(in_channels=in_ch, out_channels=(out_ch//2), kernel_size=3, padding='same'), nn.ReLU())\n",
        "        self.quarter = nn.Sequential(nn.Conv2d(in_channels=(out_ch//2), out_channels=(out_ch//4), kernel_size=3, padding='same', dilation=3), nn.ReLU())\n",
        "        self.eigth = nn.Sequential(nn.Conv2d(in_channels=(out_ch//4), out_channels=(out_ch//8), kernel_size=3, padding='same', dilation=6), nn.ReLU())\n",
        "        self.sixt1 = nn.Sequential(nn.Conv2d(in_channels=(out_ch//8), out_channels=(out_ch//16), kernel_size=3, padding='same', dilation=9), nn.ReLU())\n",
        "        self.sixt2 = nn.Sequential(nn.Conv2d(in_channels=(out_ch//16), out_channels=(out_ch//16), kernel_size=3, padding='same', dilation=12), nn.ReLU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        n2 = self.hlf(x)\n",
        "        n4 = self.quarter(n2)\n",
        "        n8 = self.eigth(n4)\n",
        "        n16_1 = self.sixt1(n8)\n",
        "        n16_2 = self.sixt2(n16_1)\n",
        "        return torch.cat([n2, n4, n8, n16_1, n16_2], dim=1)\n",
        "            \n",
        "\n",
        "class StackedDilBloc(nn.Module):\n",
        "    # similar to the U-Net approach of two convolutions per layer,\n",
        "    # it is also possible to use two SDC-blocks.\n",
        "\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        b1 = StackedDilConv(in_ch, out_ch)\n",
        "        b2 = StackedDilConv(out_ch, out_ch)\n",
        "        self.block = nn.Sequential(\n",
        "            b1,\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            b2\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class UNetStackedDilations(nn.Module):\n",
        "    # an implementation of the SDU-Net.\n",
        "    # the architecture is identical to the U-Net, except that the encoder and decoder blocks are replaced by SDC-Blocks.\n",
        "\n",
        "    def __init__(self, chs=(3,64,128,256,512,1024)):\n",
        "        super().__init__()\n",
        "        enc_chs = chs  # number of channels in the encoder\n",
        "        dec_chs = chs[::-1][:-1]  # number of channels in the decoder\n",
        "        self.enc_blocks = nn.ModuleList([StackedDilBloc(in_ch, out_ch) for in_ch, out_ch in zip(enc_chs[:-1], enc_chs[1:])])  # encoder blocks\n",
        "        self.pool = nn.MaxPool2d(2)  # pooling layer (can be reused as it will not be trained)\n",
        "        self.upconvs = nn.ModuleList([nn.ConvTranspose2d(in_ch, out_ch, 2, 2) for in_ch, out_ch in zip(dec_chs[:-1], dec_chs[1:])])  # deconvolution\n",
        "        self.dec_blocks = nn.ModuleList([StackedDilBloc(in_ch, out_ch) for in_ch, out_ch in zip(dec_chs[:-1], dec_chs[1:])])  # decoder blocks\n",
        "        self.head = nn.Sequential(nn.Conv2d(dec_chs[-1], 1, 1), nn.Sigmoid()) # 1x1 convolution for producing the output\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encode\n",
        "        enc_features = []\n",
        "        for block in self.enc_blocks[:-1]:\n",
        "            x = block(x)  # pass through the block\n",
        "            enc_features.append(x)  # save features for skip connections\n",
        "            x = self.pool(x)  # decrease resolution\n",
        "        x = self.enc_blocks[-1](x)\n",
        "        # decode\n",
        "        for block, upconv, feature in zip(self.dec_blocks, self.upconvs, enc_features[::-1]):\n",
        "            x = upconv(x)  # increase resolution\n",
        "            x = torch.cat([x, feature], dim=1)  # concatenate skip features\n",
        "            x = block(x)  # pass through the block\n",
        "        return self.head(x)  # reduce to 1 channel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_meMJcyUBho"
      },
      "outputs": [],
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "    # an implementation of attention for U-Nets, used in the skip connection.\n",
        "    # based on https://arxiv.org/abs/1804.03999\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.W_g = nn.Sequential(\n",
        "            nn.Conv2d(in_ch*2, out_ch, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(out_ch)\n",
        "            )\n",
        "        \n",
        "        self.W_x = nn.Sequential(\n",
        "            # we need a stride of 2 here: since g is taken from one level lower\n",
        "            # than the feature vector, we need to halve spatial resolution\n",
        "            # but use the same amount of channels\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=2, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(out_ch)\n",
        "        )\n",
        "\n",
        "        self.psi = nn.Sequential(\n",
        "            nn.Conv2d(out_ch, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Sigmoid(),\n",
        "            # now need to upsample the psi to get same dim as x\n",
        "            nn.ConvTranspose2d(1, 1, 2, 2)\n",
        "        )\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self,g,x):\n",
        "        g1 = self.W_g(g)\n",
        "        x1 = self.W_x(x)\n",
        "        psi = self.relu(g1+x1)\n",
        "        psi = self.psi(psi)\n",
        "\n",
        "        return x*psi\n",
        "\n",
        "\n",
        "class UNetStackedDilationsAttention(nn.Module):\n",
        "    # An Attention U-Net implementation using SDC-Blocks.\n",
        "    def __init__(self, chs=(3,64,128,256,512,1024)):\n",
        "        super().__init__()\n",
        "        enc_chs = chs  # number of channels in the encoder\n",
        "        dec_chs = chs[::-1][:-1]  # number of channels in the decoder\n",
        "        self.enc_blocks = nn.ModuleList([StackedDilBloc(in_ch, out_ch) for in_ch, out_ch in zip(enc_chs[:-1], enc_chs[1:])])  # encoder blocks\n",
        "        self.pool = nn.MaxPool2d(2)  # pooling layer (can be reused as it will not be trained)\n",
        "        self.upconvs = nn.ModuleList([nn.ConvTranspose2d(in_ch, out_ch, 2, 2) for in_ch, out_ch in zip(dec_chs[:-1], dec_chs[1:])])  # deconvolution\n",
        "        self.dec_blocks = nn.ModuleList([StackedDilBloc(in_ch, out_ch) for in_ch, out_ch in zip(dec_chs[:-1], dec_chs[1:])])  # decoder blocks\n",
        "        # attention blocks always go out_ch to out_ch/2 based on decoder blocks\n",
        "        self.att_blocks = nn.ModuleList([AttentionBlock(out_ch, out_ch//2) for out_ch in dec_chs[1:]]) # attention blocks\n",
        "        self.head = nn.Sequential(nn.Conv2d(dec_chs[-1], 1, 1), nn.Sigmoid()) # 1x1 convolution for producing the output\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encode\n",
        "        enc_features = []\n",
        "        for block in self.enc_blocks[:-1]:\n",
        "            x = block(x)  # pass through the block\n",
        "            enc_features.append(x)  # save features for skip connections\n",
        "            x = self.pool(x)  # decrease resolution\n",
        "        x = self.enc_blocks[-1](x)\n",
        "        # decode\n",
        "        for block, upconv, feature, attention in zip(self.dec_blocks, self.upconvs, enc_features[::-1], self.att_blocks):\n",
        "            # what is referred to as x in the paper is what we get from the skip connections (more spatial, less feature info)\n",
        "            # what is referred to as g comes from \"prev. layer\", so what we are actually calling x in our code (after upconv)\n",
        "            # at this point data-x is not yet upscaled --> use stride 2 (like in original paper)\n",
        "            feature = attention(g=x, x=feature)\n",
        "            x = upconv(x)  # increase resolution\n",
        "            x = torch.cat([x, feature], dim=1)  # concatenate skip features\n",
        "            x = block(x)  # pass through the block\n",
        "        return self.head(x)  # reduce to 1 channel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rHi9Eu-X7Y9"
      },
      "outputs": [],
      "source": [
        "class SkipBlock(nn.Module):\n",
        "    # implements full-scale skip connections\n",
        "    # this block is used as the decoder-block of UNet3+\n",
        "    def __init__(self, out_ch, sharedups, shareddowns):\n",
        "        super().__init__()\n",
        "        # depending on the amount of out-channels of the corresponding enc block:\n",
        "        # a different configuration of max-pools, convolutions, and up-convolutions are needed.\n",
        "        # down and up convolutions are shared, as they are not trainable.\n",
        "        gfc = GLOBAL_FIRST_CHANNELS\n",
        "        self.expectedRes = (gfc * GLOBAL_RES) // out_ch\n",
        "\n",
        "        if self.expectedRes == GLOBAL_RES:\n",
        "            # note the inclusion of a batch norm and ReLU activation after every individual convolution.\n",
        "            self.b1 = nn.Sequential(nn.Conv2d(gfc, gfc, kernel_size=3, padding=1), nn.BatchNorm2d(gfc), nn.ReLU(inplace=True))\n",
        "            self.b2 = nn.Sequential(sharedups[0], nn.Conv2d(gfc*5, gfc, kernel_size=3, padding=1), nn.BatchNorm2d(gfc), nn.ReLU(inplace=True))\n",
        "            self.b3 = nn.Sequential(sharedups[1], nn.Conv2d(gfc*4, gfc, kernel_size=3, padding=1), nn.BatchNorm2d(gfc), nn.ReLU(inplace=True))\n",
        "            self.b4 = nn.Sequential(sharedups[2], nn.Conv2d(gfc*8, gfc, kernel_size=3, padding=1), nn.BatchNorm2d(gfc), nn.ReLU(inplace=True))\n",
        "            self.b5 = nn.Sequential(sharedups[3], nn.Conv2d(gfc*16, gfc, kernel_size=3, padding=1), nn.BatchNorm2d(gfc), nn.ReLU(inplace=True))\n",
        "\n",
        "        if self.expectedRes == GLOBAL_RES // 2:\n",
        "            self.b1 = nn.Sequential(shareddowns[0], nn.Conv2d(gfc, gfc, kernel_size=3, padding=1), nn.BatchNorm2d(gfc), nn.ReLU(inplace=True))\n",
        "            self.b2 = nn.Sequential(nn.Conv2d(gfc*2, gfc, kernel_size=3, padding=1), nn.BatchNorm2d(gfc), nn.ReLU(inplace=True))\n",
        "            self.b3 = nn.Sequential(sharedups[0], nn.Conv2d(gfc*5, gfc, kernel_size=3, padding=1), nn.BatchNorm2d(gfc), nn.ReLU(inplace=True))\n",
        "            self.b4 = nn.Sequential(sharedups[1], nn.Conv2d(gfc*8, gfc, kernel_size=3, padding=1), nn.BatchNorm2d(gfc), nn.ReLU(inplace=True))\n",
        "            self.b5 = nn.Sequential(sharedups[2], nn.Conv2d(gfc*16, gfc, kernel_size=3, padding=1), nn.BatchNorm2d(gfc), nn.ReLU(inplace=True))\n",
        "        \n",
        "        if self.expectedRes == GLOBAL_RES // 4:\n",
        "            self.b1 = nn.Sequential(shareddowns[1], nn.Conv2d(gfc, gfc, kernel_size=3, padding=1), nn.BatchNorm2d(gfc), nn.ReLU(inplace=True))\n",
        "            self.b2 = nn.Sequential(shareddowns[0], nn.Conv2d(gfc*2, gfc, kernel_size=3, padding=1), nn.BatchNorm2d(gfc), nn.ReLU(inplace=True))\n",
        "            self.b3 = nn.Sequential(nn.Conv2d(gfc*4, gfc, kernel_size=3, padding=1), nn.BatchNorm2d(gfc), nn.ReLU(inplace=True))\n",
        "            self.b4 = nn.Sequential(sharedups[0], nn.Conv2d(gfc*5, gfc, kernel_size=3, padding=1), nn.BatchNorm2d(gfc), nn.ReLU(inplace=True))\n",
        "            self.b5 = nn.Sequential(sharedups[1], nn.Conv2d(gfc*16, gfc, kernel_size=3, padding=1), nn.BatchNorm2d(gfc), nn.ReLU(inplace=True))\n",
        "\n",
        "        if self.expectedRes == GLOBAL_RES // 8:\n",
        "            self.b1 = nn.Sequential(shareddowns[2], nn.Conv2d(gfc, gfc, kernel_size=3, padding=1), nn.BatchNorm2d(gfc), nn.ReLU(inplace=True))\n",
        "            self.b2 = nn.Sequential(shareddowns[1], nn.Conv2d(gfc*2, gfc, kernel_size=3, padding=1), nn.BatchNorm2d(gfc), nn.ReLU(inplace=True))\n",
        "            self.b3 = nn.Sequential(shareddowns[0], nn.Conv2d(gfc*4, gfc, kernel_size=3, padding=1), nn.BatchNorm2d(gfc), nn.ReLU(inplace=True))\n",
        "            self.b4 = nn.Sequential(nn.Conv2d(gfc*8, gfc, kernel_size=3, padding=1), nn.BatchNorm2d(gfc), nn.ReLU(inplace=True))\n",
        "            self.b5 = nn.Sequential(sharedups[0], nn.Conv2d(gfc*16, gfc, kernel_size=3, padding=1), nn.BatchNorm2d(gfc), nn.ReLU(inplace=True))\n",
        "\n",
        "        self.finalConv = nn.Sequential(nn.Conv2d(gfc*5, gfc*5, kernel_size=3, padding=1), nn.BatchNorm2d(gfc*5), nn.ReLU(inplace=True))\n",
        "\n",
        "    # full scale --> uses all skip connections\n",
        "    def forward(self, encList, prevX):\n",
        "        # we will always expect 5 entries in the encList\n",
        "        # here, need to choose which previous dec-block to forward to the current dec-block\n",
        "        if self.expectedRes == GLOBAL_RES:\n",
        "            return self.finalConv(torch.cat([self.b1(encList[0]), self.b2(prevX), self.b3(encList[2]), self.b4(encList[3]), self.b5(encList[4])], dim=1))\n",
        "\n",
        "        if self.expectedRes == GLOBAL_RES // 2:\n",
        "            return self.finalConv(torch.cat([self.b1(encList[0]), self.b2(encList[1]), self.b3(prevX), self.b4(encList[3]), self.b5(encList[4])], dim=1))\n",
        "\n",
        "        if self.expectedRes == GLOBAL_RES // 4:\n",
        "            return self.finalConv(torch.cat([self.b1(encList[0]), self.b2(encList[1]), self.b3(encList[2]), self.b4(prevX), self.b5(encList[4])], dim=1))\n",
        "\n",
        "        if self.expectedRes == GLOBAL_RES // 8:\n",
        "            return self.finalConv(torch.cat([self.b1(encList[0]), self.b2(encList[1]), self.b3(encList[2]), self.b4(encList[3]), self.b5(prevX)], dim=1))\n",
        "\n",
        "class UNet3plusRS(nn.Module):\n",
        "    # An implementation of a combined U-Net approach we refer to as U-Net3+RS\n",
        "    # SDC-Blocks are used in the encoding phase, while the above full-scale SkipBlocks are used in the decoding phase\n",
        "    def __init__(self, chs=(3,64,128,256,512,1024)):\n",
        "        super().__init__()\n",
        "        enc_chs = chs  # number of channels in the encoder\n",
        "        dec_chs = chs[::-1][:-1]  # number of channels in the decoder\n",
        "        self.enc_blocks = nn.ModuleList([StackedDilBloc(in_ch, out_ch) for in_ch, out_ch in zip(enc_chs[:-1], enc_chs[1:])])  # encoder blocks\n",
        "        self.pool = nn.MaxPool2d(2)  # pooling layer (can be reused as it will not be trained)\n",
        "\n",
        "        # the up / downsampling layers can be shared, as they are not trained\n",
        "        self.upconvs = nn.ModuleList([nn.Upsample(scale_factor = 2**i, mode='bilinear') for i in range(1, 5)])\n",
        "        self.downconvs = nn.ModuleList([nn.MaxPool2d(2**i) for i in range(1, 4)])\n",
        "\n",
        "        # we pass the shared down / upconv layers to the skip_blocks on creation and they select which ones to use in construction\n",
        "        self.skip_blocks = nn.ModuleList([SkipBlock(out_ch, self.upconvs, self.downconvs) for out_ch in dec_chs[1:]]) # skip blocks for full-scale\n",
        "        self.head = nn.Sequential(nn.Conv2d(GLOBAL_FIRST_CHANNELS*5, 1, 1), nn.Sigmoid()) # 1x1 convolution for producing the output\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encode\n",
        "        enc_features = []\n",
        "        for block in self.enc_blocks[:-1]:\n",
        "            x = block(x)  # pass through the block\n",
        "            enc_features.append(x)  # save features for skip connections\n",
        "            x = self.pool(x)  # decrease resolution\n",
        "        x = self.enc_blocks[-1](x)\n",
        "        enc_features.append(x) # need to also save last enc / dec level for full scale skips\n",
        "        # decode\n",
        "        for skipblock in self.skip_blocks:\n",
        "            # the entire decoder block is implemented within the SkipBlock\n",
        "            x = skipblock(enc_features, x)\n",
        "        return self.head(x)  # reduce to 1 channel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRU4HxIpWC9p"
      },
      "source": [
        "## Checkpoints\n",
        "These util-functions are used to enable us to store and load checkpoints of any pytorch model during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAMDni5LoGY8"
      },
      "outputs": [],
      "source": [
        "# define path for saving checkpoints\n",
        "current_time = datetime.datetime.now().strftime(\"%H%M%S\")\n",
        "checkpoint_path = f\"checkpoints/{CONFIGURATION_NAME}_{current_time}\"\n",
        "!mkdir checkpoints\n",
        "!mkdir {checkpoint_path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilBiGe5vSnSt"
      },
      "outputs": [],
      "source": [
        "def create_checkpoint(model, path, name = 'checkpoint', metadata = {}):\n",
        "    print(f\"Creating checkpoint {name}\")\n",
        "    torch.save(model.state_dict(), os.path.join(path, name + '.pt'))\n",
        "    with open(os.path.join(path, name + '.json'), 'w') as fp:\n",
        "        json.dump(metadata, fp,  indent=4)\n",
        "\n",
        "def load_checkpoint(model, path):\n",
        "    root = os.path.splitext(path)[0]\n",
        "    basename = os.path.basename(path)\n",
        "    baseroot = os.path.splitext(basename)[0]\n",
        "    dirname = os.path.dirname(path)\n",
        "    print(\"Loading checkpoint \" + baseroot)\n",
        "    # load model\n",
        "    model.load_state_dict(torch.load(root + '.pt'))\n",
        "    model.eval()\n",
        "    # load metadata\n",
        "    with open(root + '.json', 'r') as fp:\n",
        "        metadata = json.load(fp)\n",
        "    # load list of already sampled files\n",
        "    with open(dirname + '/already_sampled.list', 'r') as fp:\n",
        "        already_sampled = [f.rstrip() for f in fp.readlines()]\n",
        "    if metadata['hyperparameters'] != HYPERPARAMETERS:\n",
        "        raise Warning(\"Hyperparameters do not match!\")\n",
        "    return model, metadata, already_sampled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3heSNEIiI19"
      },
      "source": [
        "## Loss Functions\n",
        "In these cells we define various loss functions and their compounds. Note that we went through multiple iterations and ideas to arrive at the final loss functions we utilize in our report: These loss functions that are not mentioned in the report are still provided here for possible future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZ2HsB0hDd1x"
      },
      "outputs": [],
      "source": [
        "def accuracy_fn(y_hat, y):\n",
        "    # computes classification accuracy\n",
        "    return (y_hat.round() == y.round()).float().mean()\n",
        "\n",
        "\n",
        "def patch_accuracy_fn(y_hat, y):\n",
        "    # computes accuracy weighted by patches (metric used on Kaggle for evaluation)\n",
        "    h_patches = y.shape[-2] // PATCH_SIZE\n",
        "    w_patches = y.shape[-1] // PATCH_SIZE\n",
        "    patches_hat = y_hat.reshape(-1, 1, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE).mean((-1, -3)) > CUTOFF\n",
        "    patches = y.reshape(-1, 1, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE).mean((-1, -3)) > CUTOFF\n",
        "    return (patches == patches_hat).float().mean()\n",
        "\n",
        "\n",
        "def jaccard_index(y_hat, y, smooth=1):\n",
        "    # jaccard performance metric\n",
        "    intersection = (y_hat * y).sum()\n",
        "    union = y_hat.sum() + y.sum() - intersection\n",
        "    return ((intersection + smooth) / (union + smooth)).mean()\n",
        "\n",
        "\n",
        "def dice_coef(y_hat, y, smooth=1):\n",
        "    intersection = (y_hat * y).sum()\n",
        "    return (2. * intersection + smooth) / (y_hat.sum() + y.sum() + smooth)\n",
        "\n",
        "\n",
        "def soft_dice_loss(y_hat, y):\n",
        "    return 1-dice_coef(y_hat, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKwyiQsVh2pD"
      },
      "outputs": [],
      "source": [
        "# we define our own weighted BCE loss function to cope with class imbalance\n",
        "# careful: the weight parameter in the included nn.BCE just applies weighting to batch elements!\n",
        "def BCELoss_class_weighted(weights):\n",
        "\n",
        "    def loss(input, target):\n",
        "        input = torch.clamp(input,min=1e-7,max=1-1e-7)\n",
        "        bce = - weights[1] * target * torch.log(input) - (1 - target) * weights[0] * torch.log(1 - input)\n",
        "        return torch.mean(bce)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "# note: this weighting is based on ratio of 1-pixels accross all target (kaggle_data) training ground-truths\n",
        "loss_fn_weighted = BCELoss_class_weighted([1, 0.75])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfYkj79miLR1"
      },
      "outputs": [],
      "source": [
        "KERNEL_SIZE = 15\n",
        "PADDING = int((KERNEL_SIZE - 1) / 2)\n",
        "\n",
        "# sharpening and connectivity loss amplification ideas\n",
        "# these are not covered in our report, but included for future use here.\n",
        "def create_sharpening_matrix(input, target):\n",
        "    thresh1 = nn.Threshold(CUTOFF, 0.0, inplace=False)\n",
        "    input_t = thresh1(input)\n",
        "    \n",
        "    # also remove correct 1-predictions using thresholding\n",
        "    thresh2 = nn.Threshold(0.0, 0.0, inplace=False)\n",
        "    pos_if_missedzero = thresh2(-1.0 * (target - input_t))\n",
        "\n",
        "    # we want to more heavily penalize false ones that are not close to many true ones.\n",
        "    # do this by lessening the numeric value of false ones that are close to many true ones.\n",
        "    # We can use an average pooling convolution over the target to approximate the \"true oneness\" of an area.\n",
        "    # NOTE: this must produce the same dims. as the target! Using (384x384) images, by the output dim.\n",
        "    # formula we then select kernel size 15 and padding 7.\n",
        "    avg_pool = torch.nn.AvgPool2d(KERNEL_SIZE, stride=1, padding=PADDING, count_include_pad=False)\n",
        "    avg_neighbors = avg_pool(target)\n",
        "    # we can now soften penalties for false ones that are in high-oneness regions\n",
        "    penalties = pos_if_missedzero * (1 - avg_neighbors)\n",
        "\n",
        "    return torch.clamp(penalties,min=1e-7,max=1-1e-7)\n",
        "\n",
        "def create_connectivity_matrix(input, target):\n",
        "    # again, thresholding for true zeroes\n",
        "    thresh1 = nn.Threshold(CUTOFF, 0.0, inplace=False)\n",
        "    input_t = thresh1(input)\n",
        "\n",
        "    # we want to more heavily penalize false zeroes that are close to many true ones.\n",
        "    thresh2 = nn.Threshold(0.0, 0.0, inplace=False)\n",
        "    # penalties are now proportional to how far we were from having a correct one-prediction\n",
        "    # thresh removes false ones, these are handled by sharpening\n",
        "    pos_if_missedone = thresh2(target - input_t)\n",
        "\n",
        "    avg_pool = torch.nn.AvgPool2d(KERNEL_SIZE, stride=1, padding=PADDING, count_include_pad=False)\n",
        "    avg_neighbors = avg_pool(target)\n",
        "    # harden penalties for false zeroes that are in high-oneness regions\n",
        "    penalties = pos_if_missedone * avg_neighbors\n",
        "\n",
        "    return torch.clamp(penalties,min=1e-7,max=1-1e-7)\n",
        "\n",
        "def Sharpening_Loss(beta=1):\n",
        "    def sharpening(input, target):\n",
        "        return beta * torch.mean(create_sharpening_matrix(input, target)) + (1-beta) * torch.mean(create_connectivity_matrix(input, target))\n",
        "    return sharpening\n",
        "\n",
        "sharpening_loss = Sharpening_Loss(1)\n",
        "connectivity_loss = Sharpening_Loss(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f5wjLEkwroj"
      },
      "outputs": [],
      "source": [
        "# custom focal weighted BCE loss idea that was also experimented with\n",
        "# but ultimately did not improve performance.\n",
        "# the idea here is to soften penalties corresponding to how badly a pixel is\n",
        "# misclassified relative to its neighbors: e.g a missed 1 surrounded by ground-truth\n",
        "# ones should be penalized more heavily than a missed 1 surrounded by 0s.\n",
        "def Focal_BCE(gamma):\n",
        "    def focal(input, target):\n",
        "        input = torch.clamp(input,min=1e-7,max=1-1e-7)\n",
        "\n",
        "        avg_pool = torch.nn.AvgPool2d(KERNEL_SIZE, stride=1, padding=PADDING, count_include_pad=False)\n",
        "        avg_neighbors = avg_pool(target)\n",
        "\n",
        "        focal = - torch.pow(1-input, gamma) * target * torch.log(input) * (avg_neighbors) - torch.pow(input, gamma) * (1-target) * torch.log(1-input) * (1 - avg_neighbors)\n",
        "        return torch.mean(focal)\n",
        "    return focal\n",
        "loss_fn_focalbce = Focal_BCE(2)\n",
        "\n",
        "# vanilla implementation of the Focal BCE loss\n",
        "def Focal_BCE_vanilla(gamma):\n",
        "    def focalvanilla(input, target):\n",
        "        input = torch.clamp(input,min=1e-7,max=1-1e-7)\n",
        "        focal = - torch.pow(1-input, gamma) * target * torch.log(input) - torch.pow(input, gamma) * (1-target) * torch.log(1-input)\n",
        "        return torch.mean(focal)\n",
        "    return focalvanilla\n",
        "\n",
        "# compound of the dice loss with a custom Focal BCE idea\n",
        "def compound_focal_dice(beta):\n",
        "    def compound(input, target):\n",
        "        focal_bce = Focal_BCE(2)\n",
        "        return beta * focal_bce(input, target) + (1-beta) * soft_dice_loss(input, target)\n",
        "    return compound\n",
        "loss_fn_focalbce_dice = compound_focal_dice(0.5)\n",
        "\n",
        "# compound of the dice loss with the vanilla Focal BCE\n",
        "def compound_focal_dice_vanilla(beta):\n",
        "    def compoundvanilla(input, target):\n",
        "        focal_bce = Focal_BCE_vanilla(2)\n",
        "        return beta * focal_bce(input, target) + (1-beta) * soft_dice_loss(input, target)\n",
        "    return compoundvanilla\n",
        "loss_fn_focalbce_dice_vanilla = compound_focal_dice_vanilla(0.5)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDpF12aec4ND"
      },
      "outputs": [],
      "source": [
        "def chi_square_dist(y_hat, y):\n",
        "    # patched implementation of the chi squared distance used as a component of loss functions\n",
        "    h_patches = y.shape[-2] // PATCH_SIZE\n",
        "    w_patches = y.shape[-1] // PATCH_SIZE\n",
        "    patches_hat_mean = y_hat.reshape(-1, 1, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE).mean((-1, -3))\n",
        "    patches_mean = y.reshape(-1, 1, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE).mean((-1, -3))\n",
        "    return torch.mean(torch.div(torch.pow(patches_hat_mean - patches_mean, 2), patches_hat_mean + patches_mean))\n",
        "\n",
        "def compound_focal_dice_chi():\n",
        "    # a compound of dice, focal BCE and chi distance used as our final loss function.\n",
        "    def compound(input, target):\n",
        "        focal_bce = Focal_BCE_vanilla(2)\n",
        "        return focal_bce(input, target) + soft_dice_loss(input, target) + chi_square_dist(input, target)\n",
        "    return compound\n",
        "loss_fn_focal_dice_chi = compound_focal_dice_chi()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU0qo7jxiWvC"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDnV7zDPWhl1"
      },
      "source": [
        "## Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKQvTg8gE9JC"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader, eval_dataloader, model, loss_fn, metric_fns, optimizer, n_epochs, \n",
        "          train_iter=(1, 1), useEarlyStopping=USE_EARLY_STOPPING):\n",
        "    # training loop\n",
        "    history = {}  # collects metrics at the end of each epoch\n",
        "    STATE['total_iterations'] = train_iter[1]\n",
        "    STATE['current_iteration'] = train_iter[0]\n",
        "    STATE['time_trained'] = time.time() - start_model_train\n",
        "    early_stop_counter = 0\n",
        "    lowest_val_loss = 1.\n",
        "\n",
        "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        # initialize metric list\n",
        "        STATE['epoch'] = epoch+1\n",
        "        metrics = {'loss': [], 'val_loss': []}\n",
        "        for k, _ in metric_fns.items():\n",
        "            metrics[k] = []\n",
        "            metrics['val_'+k] = []\n",
        "\n",
        "        pbar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{n_epochs}')\n",
        "        # training\n",
        "        model.train()\n",
        "        for (x, y) in pbar:\n",
        "            optimizer.zero_grad()  # zero out gradients\n",
        "            y_hat = model(x)  # forward pass\n",
        "            loss = loss_fn(y_hat, y)\n",
        "            loss.backward()  # backward pass\n",
        "            optimizer.step()  # optimize weights\n",
        "\n",
        "            # log partial metrics\n",
        "            metrics['loss'].append(loss.item())\n",
        "            for k, fn in metric_fns.items():\n",
        "                metrics[k].append(fn(y_hat, y).item())\n",
        "            pbar.set_postfix({k: sum(v)/len(v) for k, v in metrics.items() if len(v) > 0})\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():  # do not keep track of gradients\n",
        "            for (x, y) in eval_dataloader:\n",
        "                y_hat = model(x)  # forward pass\n",
        "                loss = loss_fn(y_hat, y)\n",
        "                \n",
        "                # log partial metrics\n",
        "                metrics['val_loss'].append(loss.item())\n",
        "                for k, fn in metric_fns.items():\n",
        "                    metrics['val_'+k].append(fn(y_hat, y).item())\n",
        "\n",
        "        # summarize metrics and hyperparameters\n",
        "        history[epoch] = {k: sum(v) / len(v) for k, v in metrics.items()}\n",
        "        metadata = {'hyperparameters': HYPERPARAMETERS,\n",
        "                    'state': STATE,\n",
        "                    'metrics': history[epoch],\n",
        "                    }\n",
        "        print(' '.join(['\\t- '+str(k)+' = '+str(round(v, 3))+'\\n ' for (k, v) in history[epoch].items()]))\n",
        "\n",
        "        if useEarlyStopping:\n",
        "            if history[epoch]['val_loss'] < lowest_val_loss:\n",
        "                create_checkpoint(model, checkpoint_path, metadata=metadata)\n",
        "                lowest_val_loss = history[epoch]['val_loss']\n",
        "                early_stop_counter = 0\n",
        "            elif lowest_val_loss < 1.1 * history[epoch]['val_loss']:\n",
        "                early_stop_counter += 1 \n",
        "            if early_stop_counter > 10:\n",
        "                print(\"early stopping\")\n",
        "                break;\n",
        "\n",
        "        # display validation results\n",
        "        # there may be feature augmentation in x!\n",
        "        print(x.detach().cpu().numpy().shape)\n",
        "        show_val_samples(x.detach().cpu().numpy()[:,:3,:,:], y.detach().cpu().numpy(), y_hat.detach().cpu().numpy())\n",
        "\n",
        "    # end of train iteration\n",
        "    print(f'Finished training iteration: {train_iter[0]}/{train_iter[1]}')\n",
        "    print(f'Elapsed time since start of training: {round(time.time() - start_model_train, 2)} seconds')\n",
        "    model, metadata, _ = load_checkpoint(model, checkpoint_path + '/checkpoint')\n",
        "    # plot loss curves\n",
        "    plt.plot([v['loss'] for k, v in history.items()], label='Training Loss')\n",
        "    plt.plot([v['val_loss'] for k, v in history.items()], label='Validation Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    plt.plot([v['patch_acc'] for k, v in history.items()], label='Patch Acc.')\n",
        "    plt.plot([v['val_patch_acc'] for k, v in history.items()], label='Validation Patch Acc.')\n",
        "    plt.ylabel('Patch Acc.')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "    print('Datasets: ' + '; '.join(f\"{d} \" for d in DATASETS))\n",
        "    print(f'Total train set size: {train_img_set_size}')\n",
        "    print(f'BATCH_SIZE: {BATCH_SIZE}')\n",
        "    print(f'LOSS_FN: {LOSS_FN}')\n",
        "    print(' '.join(['\\t- '+str(k)+' = '+str(round(v, 3))+'\\n ' for (k, v) in history[epoch].items()]))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIvtlCJ1WdR4"
      },
      "source": [
        "## Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o95UHXRytsyH"
      },
      "outputs": [],
      "source": [
        "# if feature extraction is used, there is one additional input channel\n",
        "channels = (3,64,128,256,512,1024)\n",
        "if PREPROCESS_FRANGI:\n",
        "    channels = (4,64,128,256,512,1024)\n",
        "\n",
        "# now that all definitions have been made, we can fill in the config dictionary\n",
        "MODELS['unet'] = UNet(chs=channels)\n",
        "MODELS['sdunet'] = UNetStackedDilations(chs=channels)\n",
        "MODELS['attention_sdunet'] = UNetStackedDilationsAttention(chs=channels)\n",
        "MODELS['unet3plusRS'] = UNet3plusRS(chs=channels)\n",
        "LOSS_FNS['bce'] = nn.BCELoss()\n",
        "LOSS_FNS['soft_dice'] = soft_dice_loss\n",
        "LOSS_FNS['focal_bce'] = Focal_BCE(2)\n",
        "LOSS_FNS['custom_bce_dice_mix'] = loss_fn_focalbce_dice\n",
        "LOSS_FNS['bce_dice_mix'] = loss_fn_focalbce_dice_vanilla\n",
        "LOSS_FNS['focal_dice_chi'] = loss_fn_focal_dice_chi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Se9QQ1TIdP2t"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics\n",
        "import torchmetrics\n",
        "\n",
        "f1 = torchmetrics.F1Score(num_classes=1, threshold=0.5, average = 'weighted').to(device)\n",
        "def f1withintconv(input, target):\n",
        "    # the f1 metric needs flat input and target, and the target must be of type int.\n",
        "    return f1(input.view(-1), target.int().view(-1))\n",
        "\n",
        "model = MODELS[MODEL].to(device)\n",
        "loss_fn = LOSS_FNS[LOSS_FN]\n",
        "metric_fns = {'acc': accuracy_fn, 'patch_acc': patch_accuracy_fn, 'f1': f1withintconv}\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "start_train_iter, n_train_iter = 1, 1\n",
        "if MAX_SAMPLE_SIZE > 0:\n",
        "    n_train_iter = len(os.listdir(train_images_path)) // MAX_SAMPLE_SIZE + 1\n",
        "\n",
        "# provides the ability to resume training from a previously saved pytorch checkpoint\n",
        "if LOAD_CHECKPOINT is None:\n",
        "    already_sampled = []\n",
        "else:\n",
        "    model, metadata, already_sampled = load_checkpoint(model, LOAD_CHECKPOINT)\n",
        "    start_train_iter = metadata['state']['current_iteration'] + 1\n",
        "    start_model_train += metadata['state']['time_trained']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uepNUQ8vizzd"
      },
      "outputs": [],
      "source": [
        "start_model_train = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rBUAuUbbEeZ5"
      },
      "outputs": [],
      "source": [
        "# begin one training iteration.\n",
        "# in the case of large datasets, it is possible to run multiple training iterations\n",
        "for i in range(start_train_iter-1, n_train_iter):\n",
        "    print(f\"Start of training iteration {i+1}/{n_train_iter}\")\n",
        "    current_sample = generate_sample(train_images_path, sample_size=MAX_SAMPLE_SIZE, already_sampled=already_sampled)\n",
        "    already_sampled = sorted(already_sampled + current_sample)\n",
        "    with open(checkpoint_path + f'/already_sampled.list', \"w\") as outfile:\n",
        "        outfile.write(\"\\n\".join(already_sampled))\n",
        "    train_dataset = ImageDataset(train_path, device, use_patches=False, resize_to=(384, 384), sample=current_sample)\n",
        "    val_dataset = ImageDataset(val_path, device, use_patches=False, resize_to=(384, 384))\n",
        "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    train(train_dataloader, val_dataloader, model, loss_fn, metric_fns, optimizer, N_EPOCHS, train_iter=(i+1, n_train_iter))\n",
        "    # in the case of multiple training iterations, free up the GPU by garbage collecting\n",
        "    train_dataloader = None\n",
        "    val_dataloader = None\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T9YB8uL_0lyA"
      },
      "outputs": [],
      "source": [
        "stop_model_train = time.time()\n",
        "print(f\"Time to train model: {round(stop_model_train - start_model_train, 2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e08GFXejjlVR"
      },
      "outputs": [],
      "source": [
        "model, metadata, _ = load_checkpoint(model, checkpoint_path + '/checkpoint')\n",
        "create_checkpoint(model, checkpoint_path, CONFIGURATION_NAME + '_final_model', metadata=metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv18WaiuipH4"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5ngrf3luu_E2"
      },
      "outputs": [],
      "source": [
        "def morphological_postprocessing(imgs):\n",
        "    # an implementation of morphological postprocessing to erode solitary ones\n",
        "    result = []\n",
        "    for img in imgs:\n",
        "        kernel = np.ones((3,3), np.uint8)\n",
        "        img = img * 255\n",
        "        _, img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
        "        img = img / 255\n",
        "        img = cv2.erode(img, kernel, iterations=POSTPROCESSING_MORPHOLOGICAL_ITERATIONS)\n",
        "        img = cv2.dilate(img, kernel, iterations=POSTPROCESSING_MORPHOLOGICAL_ITERATIONS)\n",
        "        result.append(img)\n",
        "    result = np.stack(result)\n",
        "    return result\n",
        "    \n",
        "def prediction(imagesOrg, flip, rot):\n",
        "  # perform majority-voting based on multiple flipped images, as described in report\n",
        "  size = imagesOrg.shape[1:3]\n",
        "  images = np.stack([cv2.resize(img, dsize=(384, 384)) for img in imagesOrg], 0)\n",
        "  images = images[:, :, :, :3]\n",
        "\n",
        "  images_aug = []\n",
        "  for img in images:\n",
        "    \n",
        "    img_aug = img\n",
        "    for i in range(rot):\n",
        "      img_aug = np.rot90(img_aug)\n",
        "    if flip:\n",
        "      img_aug = np.flip(img_aug, axis=1)\n",
        "    images_aug.append(img_aug)\n",
        "  images_aug = np.array(images_aug)\n",
        "  \n",
        "  test_images = np_to_tensor(np.moveaxis(images_aug, -1, 1), device)\n",
        "  # the test images need to be subject to idential pre-processing\n",
        "  if PREPROCESS_FRANGI:\n",
        "    hess_img = []\n",
        "    for t in test_images:\n",
        "      with torch.no_grad():\n",
        "        grayx = np.squeeze(transforms.Grayscale().forward(t).cpu().numpy(), axis=0)\n",
        "        stackx = frangi(grayx)\n",
        "        # now stack this information (feature extraction) onto the channel dimension of x\n",
        "        tnew = torch.cat([t, torch.unsqueeze(np_to_tensor(stackx.astype(np.float32), device), 0)], dim=0)\n",
        "        hess_img.append(tnew)\n",
        "    hess_img_tensor = torch.cat([x.unsqueeze(0) for x in hess_img], dim=0)\n",
        "    test_images = hess_img_tensor\n",
        "\n",
        "  test_pred = [model(t).detach().cpu().numpy() for t in test_images.unsqueeze(1)]\n",
        "  test_pred = np.concatenate(test_pred, 0)\n",
        "  test_pred = np.moveaxis(test_pred, 1, -1)  # CHW to HWC\n",
        "  test_pred = np.stack([cv2.resize(img, dsize=size) for img in test_pred], 0)  # resize to original shape\n",
        "  if POSTPROCESSING_MORPHOLOGICAL_ITERATIONS:\n",
        "    test_pred = morphological_postprocessing(test_pred)\n",
        "  # now compute labels\n",
        "  test_pred = test_pred.reshape((-1, size[0] // PATCH_SIZE, PATCH_SIZE, size[0] // PATCH_SIZE, PATCH_SIZE))\n",
        "  test_pred = np.moveaxis(test_pred, 2, 3)\n",
        "  test_pred = np.round(np.mean(test_pred, (-1, -2)) > CUTOFF)\n",
        "  test_pred_back = []\n",
        "  for tp in test_pred:\n",
        "    tp_back = tp\n",
        "    if flip:\n",
        "      tp_back = np.flip(tp_back, axis=1)\n",
        "    for i in range(4 - rot):\n",
        "      tp_back = np.rot90(tp_back)\n",
        "    test_pred_back.append(tp_back)\n",
        "\n",
        "  test_pred_back = np.array(test_pred_back)\n",
        "\n",
        "  return test_pred_back\n",
        "\n",
        "def postprocessing_prediction(path):\n",
        "  test_filenames = (glob(path + '/*.png'))\n",
        "  test_images = load_from_path(test_path, isGroundtruth=False)\n",
        "  \n",
        "  test_preds = prediction(test_images, 0, 0)\n",
        "  if POSTPROCESSING_MAJORITY > 0:\n",
        "    for i, j in [(0, 1), (0, 2), (0, 3), (1, 0), (1, 1), (1, 2), (1, 3)]:\n",
        "      test_preds += prediction(test_images, i, j)\n",
        "    test_preds = test_preds / 8 >= 0.5\n",
        "  return test_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GruNcu-10tWO"
      },
      "outputs": [],
      "source": [
        "start_model_predict = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jB4Ui7Ma1DJA"
      },
      "outputs": [],
      "source": [
        "# load best checkpoint before predicting\n",
        "model, metadata = load_checkpoint(model, f\"{checkpoint_path}/{CONFIGURATION_NAME}_final_model\")[0:2]\n",
        "print(' '.join([f'\\t- {k} = {v}\\n ' for (k, v) in metadata['hyperparameters'].items()]))\n",
        "print(' '.join([f'\\t- {k} = {v}\\n ' for (k, v) in metadata['state'].items()]))\n",
        "print(' '.join([f'\\t- {k} = {v}\\n ' for (k, v) in metadata['metrics'].items()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1XYySeBevO4N"
      },
      "outputs": [],
      "source": [
        "test_path = 'kaggle_data/test/images'\n",
        "test_filenames = sorted(glob(test_path + '/*.png'))\n",
        "result = postprocessing_prediction(test_path)\n",
        "create_submission(result, test_filenames, submission_filename=f\"{checkpoint_path}/{CONFIGURATION_NAME}_submission.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zO2MAkY0p5qv"
      },
      "outputs": [],
      "source": [
        "# finally, download the prediction and the checkpoint for reproducibility\n",
        "# note that this was only used for our convenience in running the experiments on\n",
        "# google colab, and so this cell is only compatible with the colab platform.\n",
        "from google.colab import files\n",
        "import os\n",
        "files.download(f\"{checkpoint_path}/{CONFIGURATION_NAME}_submission.csv\")\n",
        "os.system( \"zip -r {} {}\".format( f\"{CONFIGURATION_NAME}_checkpoints.zip\" , f\"{checkpoint_path}\" ) )\n",
        "files.download(f\"{CONFIGURATION_NAME}_checkpoints.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bvm8aXqQb4PU"
      },
      "outputs": [],
      "source": [
        "# finally, check the parameters of the model\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Copy of combined_code.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}