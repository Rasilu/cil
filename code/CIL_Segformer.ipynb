{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIL_Segformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1PAYrSAjCwMAb9Ip6B3noboe2Lbz9S6bC",
      "authorship_tag": "ABX9TyPSdZFBI3jOwske6oUz/FYT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b31cb612e2f84a089a79bf544b3bd9ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2de86d13629e4a8cbbf2d0a088fc993d",
              "IPY_MODEL_e20c794a011044b8bb2ab153ade1567c",
              "IPY_MODEL_7a0db6ea8abb4b4f8edcf26641da4017"
            ],
            "layout": "IPY_MODEL_3d8c3cbe4ea04929becf50dd4ce3f1e7"
          }
        },
        "2de86d13629e4a8cbbf2d0a088fc993d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e55b0c9904e346249b09fb93840c08b7",
            "placeholder": "​",
            "style": "IPY_MODEL_3d58b38f62ce498aaf82fcf61da6e5c3",
            "value": "100%"
          }
        },
        "e20c794a011044b8bb2ab153ade1567c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c115804274d42b8868144e631f67c90",
            "max": 72,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c1e4c7a340342e2b8c577d6ae808fdb",
            "value": 72
          }
        },
        "7a0db6ea8abb4b4f8edcf26641da4017": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d3045d908934946932db63b421f5d84",
            "placeholder": "​",
            "style": "IPY_MODEL_3f8ba9429499490a8e9ee3973f91ea7d",
            "value": " 72/72 [00:42&lt;00:00,  2.31it/s]"
          }
        },
        "3d8c3cbe4ea04929becf50dd4ce3f1e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e55b0c9904e346249b09fb93840c08b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d58b38f62ce498aaf82fcf61da6e5c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c115804274d42b8868144e631f67c90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c1e4c7a340342e2b8c577d6ae808fdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d3045d908934946932db63b421f5d84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f8ba9429499490a8e9ee3973f91ea7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a1ec93e1f0a4c708a1016086660a0c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b0ece726e3e241e0ad6c25857bf16e03",
              "IPY_MODEL_be97ddf333e44aa1b06be8e259427d6a",
              "IPY_MODEL_6d1739104d4641d9b631a3c00dce3378"
            ],
            "layout": "IPY_MODEL_3f8bc8a6ae9c420bb123b6075e429ba0"
          }
        },
        "b0ece726e3e241e0ad6c25857bf16e03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_071bb31cdb454920adc019e3825ea96b",
            "placeholder": "​",
            "style": "IPY_MODEL_c1c943092007493e82922634214fb8eb",
            "value": "100%"
          }
        },
        "be97ddf333e44aa1b06be8e259427d6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_313e560546f940af9d2e03e0d0918fa3",
            "max": 72,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d291556352834ce88da96da7b9053e7c",
            "value": 72
          }
        },
        "6d1739104d4641d9b631a3c00dce3378": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2c5715b26c54d64830d5db273a84f35",
            "placeholder": "​",
            "style": "IPY_MODEL_0582118a5cb84b3a926e33ffcebb504a",
            "value": " 72/72 [00:44&lt;00:00,  2.32it/s]"
          }
        },
        "3f8bc8a6ae9c420bb123b6075e429ba0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "071bb31cdb454920adc019e3825ea96b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1c943092007493e82922634214fb8eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "313e560546f940af9d2e03e0d0918fa3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d291556352834ce88da96da7b9053e7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2c5715b26c54d64830d5db273a84f35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0582118a5cb84b3a926e33ffcebb504a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rasilu/cil/blob/main/CIL_Segformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning of the Segformer architecture\n",
        "The segformer architecture is a transformer-based architecture, describe in this paper: https://proceedings.neurips.cc/paper/2021/file/64f1f27bf1b4ec22924fd0acb550c235-Paper.pdf\n",
        "\n",
        "In this notebook we use the [huggingface](https://huggingface.co/) library for fine-tuning."
      ],
      "metadata": {
        "id": "3L03jmQ-TZ74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSzLAfPGYjrF",
        "outputId": "ba5d496e-2f47-413b-de8f-ce81a486c978"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.3.2)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.8.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import cv2\n",
        "import re\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# huggingface imports\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation, SegformerConfig\n",
        "from huggingface_hub import cached_download, hf_hub_url\n"
      ],
      "metadata": {
        "id": "bqbA73l7TgRm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions"
      ],
      "metadata": {
        "id": "_Ny9tAArXzhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# some constants\n",
        "PATCH_SIZE = 16  # pixels per side of square patches\n",
        "VAL_SIZE = 10  # size of the validation set (number of images)\n",
        "CUTOFF = 0.25  # minimum average brightness for a mask patch to be classified as containing road\n"
      ],
      "metadata": {
        "id": "IDwzPwd6YFtF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# some helper functions\n",
        "\n",
        "def np_to_tensor(x, device):\n",
        "    # allocates tensors from np.arrays\n",
        "    if device.type == 'cpu':\n",
        "        return torch.from_numpy(x).cpu()\n",
        "    else:\n",
        "        return torch.from_numpy(x).contiguous().pin_memory().to(device=device, non_blocking=True)\n",
        "\n",
        "def get_device():\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def load_all_from_path(path):\n",
        "    # loads all HxW .pngs contained in path as a 4D np.array of shape (n_images, H, W, 3)\n",
        "    # images are loaded as floats with values in the interval [0., 1.]\n",
        "    return np.stack([np.array(Image.open(f)) for f in sorted(glob(path + '/*.png'))]).astype(np.float32) / 255.\n",
        "\n",
        "def load_single_from_path(path):\n",
        "    return np.stack([np.array(Image.open(f)) for f in sorted(glob(path + '/*.png'))]).astype(np.float32) / 255.\n",
        "\n",
        "def image_to_patches(images, masks=None):\n",
        "    # takes in a 4D np.array containing images and (optionally) a 4D np.array containing the segmentation masks\n",
        "    # returns a 4D np.array with an ordered sequence of patches extracted from the image and (optionally) a np.array containing labels\n",
        "    n_images = images.shape[0]  # number of images\n",
        "    h, w = images.shape[1:3]  # shape of images\n",
        "    assert (h % PATCH_SIZE) + (w % PATCH_SIZE) == 0  # make sure images can be patched exactly\n",
        "\n",
        "    images = images[:,:,:,:3]\n",
        "    \n",
        "    h_patches = h // PATCH_SIZE\n",
        "    w_patches = w // PATCH_SIZE\n",
        "    \n",
        "    patches = images.reshape((n_images, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE, -1))\n",
        "    patches = np.moveaxis(patches, 2, 3)\n",
        "    patches = patches.reshape(-1, PATCH_SIZE, PATCH_SIZE, 3)\n",
        "    if masks is None:\n",
        "        return patches\n",
        "\n",
        "    masks = masks.reshape((n_images, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE, -1))\n",
        "    masks = np.moveaxis(masks, 2, 3)\n",
        "    labels = np.mean(masks, (-1, -2, -3)) > CUTOFF  # compute labels\n",
        "    labels = labels.reshape(-1).astype(np.float32)\n",
        "    return patches, labels\n",
        "\n",
        "\n",
        "def show_patched_image(patches, labels, h_patches=25, w_patches=25):\n",
        "    # reorders a set of patches in their original 2D shape and visualizes them\n",
        "    fig, axs = plt.subplots(h_patches, w_patches, figsize=(18.5, 18.5))\n",
        "    for i, (p, l) in enumerate(zip(patches, labels)):\n",
        "        # the np.maximum operation paints patches labeled as road red\n",
        "        axs[i // w_patches, i % w_patches].imshow(np.maximum(p, np.array([l.item(), 0., 0.])))\n",
        "        axs[i // w_patches, i % w_patches].set_axis_off()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Z4OUeDu1Ww43"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "    # Dataset class that deals with loading the data and making it available by index.\n",
        "    # This class is taken from the jupyter notebook of the tutorial session\n",
        "    def __init__(self, path, feature_extractor, device, use_patches=True, resize_to=(400, 400)):\n",
        "        self.path = path\n",
        "        self.device = device\n",
        "        self.use_patches = use_patches\n",
        "        self.resize_to=resize_to\n",
        "        self.x, self.y, self.n_samples = None, None, None\n",
        "        self._load_data()\n",
        "\n",
        "    def _load_data(self):  # not very scalable, but good enough for now\n",
        "        self.x = load_all_from_path(os.path.join('data', self.path, 'images'))[:,:,:,:3]\n",
        "        self.y = load_all_from_path(os.path.join('data', self.path, 'groundtruth'))\n",
        "        if self.use_patches:  # split each image into patches\n",
        "            self.x, self.y = image_to_patches(self.x, self.y)\n",
        "        elif self.resize_to != (self.x.shape[1], self.x.shape[2]):  # resize images\n",
        "            self.x = np.stack([cv2.resize(img, dsize=self.resize_to) for img in self.x], 0)\n",
        "            self.y = np.stack([cv2.resize(mask, dsize=self.resize_to) for mask in self.y], 0)\n",
        "        self.x = np.moveaxis(self.x, -1, 1)  # pytorch works with CHW format instead of HWC\n",
        "        self.n_samples = len(self.x)\n",
        "\n",
        "    def _preprocess(self, x, y):\n",
        "        # to keep things simple we will not apply transformations to each sample,\n",
        "        # but it would be a very good idea to look into preprocessing\n",
        "        # TODO: apply some transformations\n",
        "        return x, y\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self._preprocess(np_to_tensor(self.x[item], self.device), np_to_tensor(self.y[[item]], self.device))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.n_samples\n"
      ],
      "metadata": {
        "id": "AkE37WdOWr7H"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SemanticSegmentationDataset(Dataset):\n",
        "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
        "    # Taken from: https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb\n",
        "\n",
        "    def __init__(self, root_dir, feature_extractor, train=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
        "            feature_extractor (SegFormerFeatureExtractor): feature extractor to prepare images + segmentation maps.\n",
        "            train (bool): Whether to load \"training\" or \"validation\" images + annotations.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.train = train\n",
        "\n",
        "        sub_path = \"training\" if self.train else \"validation\"\n",
        "        self.img_dir = os.path.join(self.root_dir, sub_path, \"images\")\n",
        "        self.ann_dir = os.path.join(self.root_dir, sub_path, \"groundtruth\")\n",
        "\n",
        "        #self.img_dir = os.path.join(self.root_dir, \"images\", sub_path)\n",
        "        #self.ann_dir = os.path.join(self.root_dir, \"annotations\", sub_path)\n",
        "\n",
        "        # read images\n",
        "        image_file_names = []\n",
        "        for root, dirs, files in os.walk(self.img_dir):\n",
        "          image_file_names.extend(files)\n",
        "        self.images = sorted(image_file_names)\n",
        "\n",
        "        # read annotations\n",
        "        annotation_file_names = []\n",
        "        for root, dirs, files in os.walk(self.ann_dir):\n",
        "          annotation_file_names.extend(files)\n",
        "        self.annotations = sorted(annotation_file_names)\n",
        "\n",
        "        assert len(self.images) == len(self.annotations), \"There must be as many images as there are segmentation maps\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(os.path.join(self.img_dir, self.images[idx])).convert('RGB')\n",
        "        segmentation_map = Image.open(os.path.join(self.ann_dir, self.annotations[idx]))\n",
        "        segmentation_map = (np.array(segmentation_map) == 255).astype(int)\n",
        "\n",
        "        # randomly crop + pad both image and segmentation map to same size\n",
        "        encoded_inputs = self.feature_extractor(image, segmentation_map, return_tensors=\"pt\")\n",
        "\n",
        "        for k, v in encoded_inputs.items():\n",
        "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
        "\n",
        "        return encoded_inputs\n"
      ],
      "metadata": {
        "id": "iwkUVBrYT7Cs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = SegformerFeatureExtractor(reduce_labels=True)\n",
        "\n",
        "device = get_device()\n",
        "\n",
        "# load the data\n",
        "root_dir = os.path.join('drive', 'MyDrive', 'ETH', 'CIL', 'Project', 'datasets', 'kaggle_data')\n",
        "\n",
        "\n",
        "feature_extractor = SegformerFeatureExtractor(reduce_labels=True, size=400)\n",
        "\n",
        "train_dataset = SemanticSegmentationDataset(root_dir=root_dir, feature_extractor=feature_extractor)\n",
        "valid_dataset = SemanticSegmentationDataset(root_dir=root_dir, feature_extractor=feature_extractor, train=False)\n",
        "encoded_inputs = train_dataset[0]\n",
        "\n",
        "print(f\"shape of pixel_values: {encoded_inputs['pixel_values'].shape}\")\n",
        "print(f\"shape of labels: {encoded_inputs['labels'].shape}\")\n",
        "\n",
        "print(f\"labels: {encoded_inputs['labels']}\")\n",
        "\n",
        "print(f\"Unique labels: {encoded_inputs['labels'].squeeze().unique()}\")\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=2)\n",
        "\n",
        "\n",
        "# id2label mapping\n",
        "id2label = {0: \"is_not_road\", 1: \"is_road\"}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "# define model\n",
        "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b5\",\n",
        "                                                          num_labels=2,\n",
        "                                                          id2label=id2label,\n",
        "                                                          label2id=label2id)\n",
        "\n",
        "metric = load_metric(\"mean_iou\")\n",
        "# fine-tuning\n",
        "\n",
        "# define optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00006)\n",
        "model.to(device)\n",
        "\n",
        "model.train()\n",
        "\n",
        "history = {}  # collects metrics at the end of each epoch\n",
        "n_epochs = 2\n",
        "\n",
        "n_accumulation_steps = 8 # accumulating gradients, together with a batch size of 2 we update the gradients every 16 samples\n",
        "\n",
        "optimizer.zero_grad()\n",
        "\n",
        "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "   print(\"Epoch:\", epoch)\n",
        "   for idx, batch in enumerate(tqdm(train_dataloader)):\n",
        "        # get the inputs;\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "  \n",
        "        # forward + backward + optimize\n",
        "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "        loss, logits = outputs.loss, outputs.logits\n",
        "        loss = loss / n_accumulation_steps\n",
        "        loss.backward()\n",
        "\n",
        "        if idx % n_accumulation_steps == 0: # this is for the accumulating gradients\n",
        "          optimizer.step()\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "          # evaluate\n",
        "          with torch.no_grad():\n",
        "            upsampled_logits = nn.functional.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "            predicted = upsampled_logits.argmax(dim=1)\n",
        "            \n",
        "            # note that the metric expects predictions + labels as numpy arrays\n",
        "            metric.add_batch(predictions=predicted.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
        "\n",
        "\n",
        "        # let's print loss and metrics every 100 batches\n",
        "        if idx % 100 == 0:\n",
        "          metrics = metric.compute(num_labels=len(id2label), \n",
        "                                   ignore_index=None,\n",
        "                                   reduce_labels=False, # we've already reduced the labels before)\n",
        "          )\n",
        "\n",
        "          print(\"Loss:\", loss.item())\n",
        "          print(\"Mean_iou:\", metrics[\"mean_iou\"])\n",
        "          print(\"Mean accuracy:\", metrics[\"mean_accuracy\"])\n",
        "\n",
        "# save the fine-tuned model\n",
        "model_dir = \"fine_tuned_segformer\"\n",
        "os.makedirs(model_dir)\n",
        "model.save_pretrained(model_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560,
          "referenced_widgets": [
            "b31cb612e2f84a089a79bf544b3bd9ea",
            "2de86d13629e4a8cbbf2d0a088fc993d",
            "e20c794a011044b8bb2ab153ade1567c",
            "7a0db6ea8abb4b4f8edcf26641da4017",
            "3d8c3cbe4ea04929becf50dd4ce3f1e7",
            "e55b0c9904e346249b09fb93840c08b7",
            "3d58b38f62ce498aaf82fcf61da6e5c3",
            "5c115804274d42b8868144e631f67c90",
            "5c1e4c7a340342e2b8c577d6ae808fdb",
            "0d3045d908934946932db63b421f5d84",
            "3f8ba9429499490a8e9ee3973f91ea7d",
            "7a1ec93e1f0a4c708a1016086660a0c0",
            "b0ece726e3e241e0ad6c25857bf16e03",
            "be97ddf333e44aa1b06be8e259427d6a",
            "6d1739104d4641d9b631a3c00dce3378",
            "3f8bc8a6ae9c420bb123b6075e429ba0",
            "071bb31cdb454920adc019e3825ea96b",
            "c1c943092007493e82922634214fb8eb",
            "313e560546f940af9d2e03e0d0918fa3",
            "d291556352834ce88da96da7b9053e7c",
            "b2c5715b26c54d64830d5db273a84f35",
            "0582118a5cb84b3a926e33ffcebb504a"
          ]
        },
        "id": "vEY5wL49ZpbO",
        "outputId": "fa120778-f986-47ac-f259-96d1c13433e6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of pixel_values: torch.Size([3, 400, 400])\n",
            "shape of labels: torch.Size([400, 400])\n",
            "labels: tensor([[255, 255, 255,  ..., 255, 255, 255],\n",
            "        [255, 255, 255,  ..., 255, 255, 255],\n",
            "        [255, 255, 255,  ..., 255, 255, 255],\n",
            "        ...,\n",
            "        [255, 255, 255,  ...,   0,   0,   0],\n",
            "        [255, 255, 255,  ...,   0,   0,   0],\n",
            "        [255, 255, 255,  ...,   0,   0,   0]])\n",
            "Unique labels: tensor([  0, 255])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at nvidia/mit-b5 were not used when initializing SegformerForSemanticSegmentation: ['classifier.bias', 'classifier.weight']\n",
            "- This IS expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b5 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.weight', 'decode_head.linear_fuse.weight', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.0.proj.bias', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.classifier.bias', 'decode_head.batch_norm.running_var', 'decode_head.linear_c.3.proj.bias', 'decode_head.classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/72 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b31cb612e2f84a089a79bf544b3bd9ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/huggingface/modules/datasets_modules/metrics/mean_iou/d4add40cf977cdd73590b5873fa830f3f13adb678f6777a29fb07b7c81d14342/mean_iou.py:259: RuntimeWarning: invalid value encountered in true_divide\n",
            "  acc = total_area_intersect / total_area_label\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.09426911175251007\n",
            "Mean_iou: 0.049631587026600296\n",
            "Mean accuracy: 0.439061060187206\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/72 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a1ec93e1f0a4c708a1016086660a0c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0108725531026721\n",
            "Mean_iou: 0.11605453291881576\n",
            "Mean accuracy: 0.8012128675258963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "bWg4zMFtnONC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.image as mpimg\n",
        "import PIL\n",
        "\n",
        "\n",
        "base_dir = os.path.join('drive', 'MyDrive', 'ETH', 'CIL', 'Project', 'datasets', 'kaggle_data', 'test', 'images')\n",
        "submission_filename = \"segformer_submission.csv\"\n",
        "\n",
        "foreground_threshold = 0.25 # percentage of pixels of val 255 required to assign a foreground label to a patch\n",
        "\n",
        "# assign a label to a patch\n",
        "def patch_to_label(patch):\n",
        "    patch = patch.astype(np.float64) / 255\n",
        "    df = np.mean(patch)\n",
        "    if df > foreground_threshold:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def mask_to_submission_strings(image_filename, mask_dir=None):\n",
        "    \"\"\"Reads a single image and outputs the strings that should go into the submission file\"\"\"\n",
        "    img_number = int(re.search(r\"\\d+\", image_filename).group(0))\n",
        "    im = PIL.Image.open(image_filename)\n",
        "    im_arr = np.asarray(im)\n",
        "    if len(im_arr.shape) > 2:\n",
        "        # Convert to grayscale.\n",
        "        im = im.convert(\"L\")\n",
        "        im_arr = np.asarray(im)\n",
        "\n",
        "    patch_size = 16\n",
        "    mask = np.zeros_like(im_arr)\n",
        "    for j in range(0, im_arr.shape[1], patch_size):\n",
        "        for i in range(0, im_arr.shape[0], patch_size):\n",
        "            patch = im_arr[i:i + patch_size, j:j + patch_size]\n",
        "            label = patch_to_label(patch)\n",
        "            mask[i:i+patch_size, j:j+patch_size] = int(label*255)\n",
        "            yield(\"{:03d}_{}_{},{}\".format(img_number, j, i, label))\n",
        "\n",
        "    if mask_dir:\n",
        "        save_mask_as_img(mask, os.path.join(mask_dir, \"mask_\" + image_filename.split(\"/\")[-1]))\n",
        "    \n",
        "\n",
        "def save_mask_as_img(img_arr, mask_filename):\n",
        "    img = PIL.Image.fromarray(img_arr)\n",
        "    os.makedirs(os.path.dirname(mask_filename), exist_ok=True)\n",
        "    img.save(mask_filename)\n",
        "\n",
        "\n",
        "def masks_to_submission(submission_filename, mask_dir, *image_filenames):\n",
        "    \"\"\"Converts images into a submission file\"\"\"\n",
        "    with open(submission_filename, 'w') as f:\n",
        "        f.write('id,prediction\\n')\n",
        "        for fn in image_filenames[0:]:\n",
        "            f.writelines('{}\\n'.format(s) for s in mask_to_submission_strings(fn, mask_dir=mask_dir))\n",
        "\n",
        "def main():\n",
        "    image_filenames = [os.path.join(base_dir, name) for name in os.listdir(base_dir)]\n",
        "    masks_to_submission(submission_filename, \"\", *image_filenames)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "J1dgbAa4jVBE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krSMoh9lmVSW",
        "outputId": "af1ff226-92a4-4d70-a839-1ee8cde4390e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1188\n",
            "drwx------ 5 root root    4096 Jul  5 13:16 drive\n",
            "drwxr-xr-x 2 root root    4096 Jul  5 14:42 fine_tuned_segformer\n",
            "drwxr-xr-x 1 root root    4096 Jun 29 13:44 sample_data\n",
            "-rw-r--r-- 1 root root 1202414 Jul  5 14:44 segformer_submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "plf1t8rDv7AM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}